<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Ryan Murphy</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <div class="page-layout">

        <div id="sidebar"></div>

        <script>
            fetch('sidebar.html')
                .then(res => res.text())
                .then(html => {
                    document.getElementById('sidebar').innerHTML = html;
                });
        </script>

        <div class="main-content">
            <div id="site-header"></div>

            <script>
                fetch('header.html')
                    .then(res => res.text())
                    .then(html => {
                        document.getElementById('site-header').innerHTML = html;
                    });
            </script>

            <main class="container">

                <p>
                    This past summer, I conducted a research apprenticeship on multi-agent deep reinforcement learning
                    (multi-agent DRL) algorithms. The goal is to apply these algorithms to partially observable games. This is a class
                    of games which the agent cannot observe the full state, but only a part of it. It is easy to
                    imagine the applicability of these scenarios, but the area is thus far understudied. I am
                    conducting these studies under
                    <a href="https://pages.cs.wisc.edu/~yw/" target="_blank">Dr. Young Wu</a> and am being funded
                    through the L&S Honors Program.
                </p>

                <section class="projects">

                    <article class="project">
                        <div class="project-image">
                            <video autoplay muted loop>
                                <source src="assets/videos/QPlanning.mp4" type="video/mp4">
                                Fictitious Play
                            </video>
                        </div>
                        <div class="project-details">
                            <h2><a href="https://github.com/rmurphy120/FictitiousPlay" target="_blank" class="clean-link">Fictitious Play</a></h2>
                            <p class="tags">Java</p>
                            <p>
                                Algorithm to solve for an exact Nash equilibrium of a zero-sum markov game. Set up for
                                any k number of cars in a pursuit-evation game on an m by n board. Has support for
                                multiple action spaces, including games with and without an action to do nothing.
                                Because this solves for the exact equilibrium, it is the base to which I will compare
                                subsequent approximation algorithms to. Credit for visualization:
                                <a href="https://ghadcock.github.io/298-test/" target="_blank">Glenn Hadcock</a>
                            </p>
                        </div>
                    </article>

                    <article class="project">
                        <div class="project-image">
                            <img src="assets/images/A2CRPS.png" alt="A2C policy learned over time">
                        </div>
                        <div class="project-details">
                            <h2><a href="https://github.com/rmurphy120/PolicyGradientAlgorithms" target="_blank" class="clean-link">Policy Gradient Algorithms</a></h2>
                            <p class="tags">Python Â· PyTorch</p>
                            <p>
                                This is a class of DRL algorithms which approximates the Nash equilibrium based on the mulit-agent policy gradient theorem. 
                                I implemented three of these algorithms: REINFORCE, A2C, and SAC. They're powerful tools when 
                                there exists a pure equilibrium strategy at all states (So they're great at single agent problems), but they struggle if there 
                                are states with only mixed equilibrium strategies. This is because these algorithms train best responses and also aren't very stable.
                                REINFORCE is the original policy gradient algorithms. A2C and SAC tries to address the stability of training. Check out the GitHub repo 
                                for more details on each algorithm. The graph on the left plots the policy learned by A2C over time while playing rock, paper, scissors. 
                                It gives a good intuition how these algorithms are driven by best response dynamics and struggle to converge to mixed strategies.
                            </p>
                        </div>
                    </article>

                    <article class="project">
                        <div class="project-image">
                            <img src="assets/images/ProgressBar.png" alt="In Progress">
                        </div>
                        <div class="project-details">
                            <p>
                                Come back later in the summer for updates!
                            </p>
                        </div>
                    </article>

                </section>
            </main>
        </div>
    </div>
</body>

</html>