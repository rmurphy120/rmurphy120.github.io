<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Ryan Murphy</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <div class="page-layout">

        <div id="sidebar"></div>

        <script>
            fetch('sidebar.html')
                .then(res => res.text())
                .then(html => {
                    document.getElementById('sidebar').innerHTML = html;
                });
        </script>

        <div class="main-content">
            <div id="site-header"></div>

            <script>
                fetch('header.html')
                    .then(res => res.text())
                    .then(html => {
                        document.getElementById('site-header').innerHTML = html;
                    });
            </script>

            <main class="container">

                <p>
                    This past summer, I conducted a research apprenticeship on multi-agent deep reinforcement learning
                    (multi-agent DRL) algorithms. The goal is to apply these algorithms to partially observable games.
                    This is a class
                    of games which the agent cannot observe the full state, but only a part of it. It is easy to
                    imagine the applicability of these scenarios, but the area is thus far understudied. I am
                    conducting these studies under
                    <a href="https://pages.cs.wisc.edu/~yw/" target="_blank">Dr. Young Wu</a> and am being funded
                    through the L&S Honors Program.
                </p>

                <section class="projects">

                    <article class="project">
                        <div class="project-image">
                            <video autoplay muted loop>
                                <source src="assets/videos/QPlanning.mp4" type="video/mp4">
                                Fictitious Play
                            </video>
                        </div>
                        <div class="project-details">
                            <h2><a href="https://github.com/rmurphy120/FictitiousPlay" target="_blank"
                                    class="clean-link">Fictitious Play</a></h2>
                            <p class="tags">Java</p>
                            <p>
                                Algorithm to solve for an exact Nash equilibrium of a zero-sum markov game. Set up for
                                any k number of cars in a pursuit-evation game on an m by n board. Has support for
                                multiple action spaces, including games with and without an action to do nothing.
                                Because this solves for the exact equilibrium, it is the base to which I will compare
                                subsequent approximation algorithms to. Credit for visualization:
                                <a href="https://ghadcock.github.io/298-test/" target="_blank">Glenn Hadcock</a>
                            </p>
                        </div>
                    </article>

                    <article class="project">
                        <div class="project-image">
                            <img src="assets/images/A2CRPS.png" alt="A2C policy learned over time">
                        </div>
                        <div class="project-details">
                            <h2><a href="https://github.com/rmurphy120/PolicyGradientAlgorithms" target="_blank"
                                    class="clean-link">Policy Gradient Algorithms</a></h2>
                            <p class="tags">Python · PyTorch</p>
                            <p>
                                This is a class of DRL algorithms which approximates the Nash equilibrium based on the
                                mulit-agent policy gradient theorem.
                                I implemented three of these algorithms: REINFORCE, A2C, and SAC. They're powerful tools
                                when
                                there exists a pure equilibrium strategy at all states (So they're great at single agent
                                problems), but they struggle if there
                                are states with only mixed equilibrium strategies. This is because these algorithms
                                train best responses and also aren't very stable.
                                REINFORCE is the original policy gradient algorithms. A2C and SAC tries to address the
                                stability of training. Check out the GitHub repo
                                for more details on each algorithm. The graph on the left plots the policy learned by
                                A2C over time while playing rock, paper, scissors.
                                It gives a good intuition how these algorithms are driven by best response dynamics and
                                struggle to converge to mixed equilibria (The mixed
                                equilibrium of rock, paper, scissors being playing each move with equal probability).
                            </p>
                        </div>
                    </article>

                    <article class="project">
                        <div class="project-image">
                            <img src="assets/images/MvsNMFixedM.png" alt="Markovian vs non-Markovian REINFORCE">
                        </div>
                        <div class="project-details">
                            <h2><a href="https://github.com/rmurphy120/PolicyGradientAlgorithms" target="_blank"
                                    class="clean-link">Non-Markovian Policy Gradient Algorithms</a></h2>
                            <p class="tags">Python · PyTorch</p>
                            <p>
                                These algorthims extend the policy gradient algorithms to use non-Markovian policies,
                                which can be used in partially observable environments.
                                I implemented the non-Markovian REINFORCE algorithm using an LSTM and compared it to the
                                orgininal Markovian REINFORCE algorithm. The graph on
                                the left shows the results of this comparison (see the GitHub repo for more details). To
                                summarize though, this shows no dip in performance when
                                using the non-Markovian policy, which gives credance to the use of these algorthims to
                                solve partially observable games. The reason
                                we never tested these algorithms in a partially observable game is because there is no
                                way to benchmark their performance. This is beacuse there aren't
                                efficient way to solve for the exact Nash equilibrium of a partially observable game.
                            </p>
                        </div>
                    </article>

                </section>
            </main>
        </div>
    </div>
</body>

</html>